import * as cheerio from 'cheerio';

type ScrapeOut = {
  url: string;
  title: string;
  description: string;
  headings: string[];
  text: string;
};

function normalizeUrl(domain: string) {
  let url = domain.trim();
  if (!/^https?:\/\//i.test(url)) url = `https://${url}`;
  return url;
}

export async function scrapeDomain(domain: string): Promise<ScrapeOut> {
  const url = normalizeUrl(domain);
  try {
    const res = await fetch(url, {
      headers: { 'User-Agent': 'AI-Qualifier/1.0 (+https://example.com)' },
      cache: 'no-store',
    });

    const html = await res.text();
    const $ = cheerio.load(html);

    const title = $('title').first().text().trim() || domain;
    const description = $('meta[name="description"]').attr('content')?.trim() || '';
    const headings = $('h1, h2')
      .map((_, el) => $(el).text().trim())
      .get()
      .filter(Boolean)
      .slice(0, 12);

    const paras = $('p')
      .map((_, el) => $(el).text().trim())
      .get()
      .filter(Boolean)
      .join('\n')
      .slice(0, 9000);

    const text = [title, description, headings.join(' | '), paras].join('\n');
    return { url, title, description, headings, text };
  } catch {
    // Resilient fallback (works even if fetch fails)
    return {
      url,
      title: `Site: ${domain}`,
      description: `Autogenerated fallback summary for ${domain}.`,
      headings: [],
      text: `No HTML fetched for ${domain}.`,
    };
  }
}
